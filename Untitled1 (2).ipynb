{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a75d36cb",
   "metadata": {},
   "source": [
    "naive bayes, prob hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0956a808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08\n",
      "8.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "cards = 52 \n",
    "aces = 4\n",
    "\n",
    "ace_probability = aces / cards\n",
    "\n",
    "print(round(ace_probability, 2))\n",
    "\n",
    "ace_probability_percent = ace_probability * 100\n",
    "\n",
    "print(str(round(ace_probability_percent, 0)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0162fccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Heart :-  25.0%\n",
      "Probability of Face Card :-  23.1%\n",
      "Probability of Queen of Hearts :-  1.9%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "def event_probability(event_outcomes, sample_space):\n",
    "    probability = (event_outcomes / sample_space) *100\n",
    "    return round(probability, 1)\n",
    "# Sample Space\n",
    "cards = 52\n",
    "# Determine the probability of drawing a heart\n",
    "hearts = 13\n",
    "heart_probability = event_probability(hearts, cards)\n",
    "# Determine the probability of drawing a face card\n",
    "face_cards = 12\n",
    "face_card_probability = event_probability(face_cards, cards)\n",
    "# Determine the probability of drawing the queen of hearts\n",
    "queen_of_hearts = 1\n",
    "queen_of_hearts_probability = event_probability(queen_of_hearts, cards)\n",
    "# Print each probability\n",
    "print(\"Probability of Heart :- \",str(heart_probability) + '%')\n",
    "print(\"Probability of Face Card :- \",str(face_card_probability) + '%')\n",
    "print(\"Probability of Queen of Hearts :- \",str(queen_of_hearts_probability) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a564b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pomegranate import *\n",
    "# Initially the door selected by the guest is completely random\n",
    "guest =DiscreteDistribution( { 'A': 1./3, 'B': 1./3, 'C': 1./3 } )\n",
    "# The door containing the prize is also a random process\n",
    "prize =DiscreteDistribution( { 'A': 1./3, 'B': 1./3, 'C': 1./3 } )\n",
    "# The door Monty picks\n",
    "monty =ConditionalProbabilityTable( [[ 'A',\n",
    "'A', 'A', 0.0 ],\n",
    "[ 'A', 'A', 'B', 0.5 ],\n",
    "[ 'A', 'A', 'C', 0.5 ],\n",
    "[ 'A', 'B', 'A', 0.0 ],\n",
    "[ 'A', 'B', 'B', 0.0 ],\n",
    "[ 'A', 'B', 'C', 1.0 ],\n",
    "[ 'A', 'C', 'A', 0.0 ],\n",
    "[ 'A', 'C', 'B', 1.0 ],\n",
    "\n",
    "[ 'A', 'C', 'C', 0.0 ],\n",
    "[ 'B', 'A', 'A', 0.0 ],\n",
    "[ 'B', 'A', 'B', 0.0 ],\n",
    "[ 'B', 'A', 'C', 1.0 ],\n",
    "[ 'B', 'B', 'A', 0.5 ],\n",
    "[ 'B', 'B', 'B', 0.0 ],\n",
    "[ 'B', 'B', 'C', 0.5 ],\n",
    "[ 'B', 'C', 'A', 1.0 ],\n",
    "[ 'B', 'C', 'B', 0.0 ],\n",
    "[ 'B', 'C', 'C', 0.0 ],\n",
    "[ 'C', 'A', 'A', 0.0 ],\n",
    "[ 'C', 'A', 'B', 1.0 ],\n",
    "[ 'C', 'A', 'C', 0.0 ],\n",
    "[ 'C', 'B', 'A', 1.0 ],\n",
    "[ 'C', 'B', 'B', 0.0 ],\n",
    "[ 'C', 'B', 'C', 0.0 ],\n",
    "[ 'C', 'C', 'A', 0.5 ],\n",
    "[ 'C', 'C', 'B', 0.5 ],\n",
    "[ 'C', 'C', 'C', 0.0 ]], [guest, prize] )\n",
    "d1 = State( guest, name=\"guest\" )\n",
    "d2 = State( prize, name=\"prize\" )\n",
    "d3 = State( monty, name=\"monty\" )\n",
    "#Building the Bayesian Network\n",
    "network = BayesianNetwork( \"Solving the Monty Hall Problem With Bayesian Networks\" )\n",
    "network.add_states(d1, d2, d3)\n",
    "network.add_edge(d1, d3)\n",
    "network.add_edge(d2, d3)\n",
    "network.bake()\n",
    "beliefs = network.predict_proba({ 'guest' : 'A' })\n",
    "beliefs = map(str, beliefs)\n",
    "print(\"n\".join( \"{}t{}\".format( state.name, belief ) for state, belief in zip( network.states, beliefs ) ))\n",
    "beliefs = network.predict_proba({'guest' : 'A', 'monty' : 'B'})\n",
    "print(\"n\".join( \"{}t{}\".format( state.name, str(belief) )\n",
    "for state, belief in zip( network.states, beliefs )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pomegranate import *\n",
    "\n",
    "# Define the states\n",
    "burglary = DiscreteDistribution({'True': 0.001, 'False': 0.999})\n",
    "earthquake = DiscreteDistribution({'True': 0.002, 'False': 0.998})\n",
    "alarm = ConditionalProbabilityTable(\n",
    "[['True', 'True', 'True', 0.95],\n",
    "\n",
    "['True', 'True', 'False', 0.05],\n",
    "['True', 'False', 'True', 0.94],\n",
    "['True', 'False', 'False', 0.06],\n",
    "['False', 'True', 'True', 0.29],\n",
    "['False', 'True', 'False', 0.71],\n",
    "['False', 'False', 'True', 0.001],\n",
    "['False', 'False', 'False', 0.999]],\n",
    "[burglary, earthquake]\n",
    ")\n",
    "david_calls = ConditionalProbabilityTable(\n",
    "[['True', 'True', 0.9],\n",
    "['True', 'False', 0.1],\n",
    "['False', 'True', 0.05],\n",
    "['False', 'False', 0.95]],\n",
    "[alarm]\n",
    ")\n",
    "sophia_calls = ConditionalProbabilityTable(\n",
    "[['True', 'True', 0.7],\n",
    "['True', 'False', 0.3],\n",
    "['False', 'True', 0.01],\n",
    "['False', 'False', 0.99]],\n",
    "[alarm]\n",
    ")\n",
    "\n",
    "s1 = State(burglary, name=\"burglary\")\n",
    "s2 = State(earthquake, name=\"earthquake\")\n",
    "s3 = State(alarm, name=\"alarm\")\n",
    "s4 = State(david_calls, name=\"david_calls\")\n",
    "s5 = State(sophia_calls, name=\"sophia_calls\")\n",
    "\n",
    "network = BayesianNetwork(\"Burglary Alarm\")\n",
    "network.add_states(s1, s2, s3, s4, s5)\n",
    "network.add_edge(s1, s3)\n",
    "network.add_edge(s2, s3)\n",
    "network.add_edge(s3, s4)\n",
    "network.add_edge(s3, s5)\n",
    "network.bake()\n",
    "\n",
    "# Calculate the probability of the alarm given burglary\n",
    "prob_alarm_given_burglary = network.predict_proba({'burglary': 'False'})[2].parameters[0]['True']\n",
    "print(\"Probability of Alarm given Burglary:\", prob_alarm_given_burglary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5afcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Define the Bayesian network structure\n",
    "model = BayesianNetwork([('Attendance', 'Exam'),\n",
    "                         ('StudyTime', 'Exam'),\n",
    "                         ('PreviousScores', 'Exam')])\n",
    "\n",
    "# Define conditional probability distributions (CPDs)\n",
    "cpd_attendance = TabularCPD(variable='Attendance', variable_card=2,\n",
    "                            values=[[0.8], [0.2]],\n",
    "                            state_names={'Attendance': ['High', 'Low']})\n",
    "\n",
    "cpd_study_time = TabularCPD(variable='StudyTime', variable_card=2,\n",
    "                             values=[[0.7], [0.3]],\n",
    "                             state_names={'StudyTime': ['High', 'Low']})\n",
    "\n",
    "cpd_previous_scores = TabularCPD(variable='PreviousScores', variable_card=2,\n",
    "                                  values=[[0.9], [0.1]],\n",
    "                                  state_names={'PreviousScores': ['Good', 'Poor']})\n",
    "\n",
    "cpd_exam = TabularCPD(variable='Exam', variable_card=2,\n",
    "                      values=[[0.95, 0.8, 0.7, 0.4, 0.9, 0.6, 0.5, 0.2],\n",
    "                              [0.05, 0.2, 0.3, 0.6, 0.1, 0.4, 0.5, 0.8]],\n",
    "                      evidence=['Attendance', 'StudyTime', 'PreviousScores'],\n",
    "                      evidence_card=[2, 2, 2],\n",
    "                      state_names={'Exam': ['Pass', 'Fail'],\n",
    "                                   'Attendance': ['High', 'Low'],\n",
    "                                   'StudyTime': ['High', 'Low'],\n",
    "                                   'PreviousScores': ['Good', 'Poor']})\n",
    "\n",
    "# Add CPDs to the model\n",
    "model.add_cpds(cpd_attendance, cpd_study_time, cpd_previous_scores, cpd_exam)\n",
    "\n",
    "# Perform inference\n",
    "inference = VariableElimination(model)\n",
    "\n",
    "# Calculate probability of passing the exam given high attendance and high study time\n",
    "result = inference.query(variables=['Exam'], evidence={'Attendance': 'High', 'StudyTime': 'High'})\n",
    "pass_prob = result.values[1]  # Probability of passing the exam\n",
    "print(\"Probability of passing the exam given high attendance and high study time:\", pass_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679cfc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "inference = VariableElimination(model)\n",
    "\n",
    "result_1 = inference.query(variables=['Performance'], evidence={'StudyTime': 1, 'Tutoring': 1})\n",
    "print(\"Probability of poor performance given medium study time and access to tutoring:\", result_1.values[0])\n",
    "\n",
    "result_2 = inference.query(variables=['StudyTime', 'Tutoring'], evidence={'Performance': 2})\n",
    "print(\"Probability distribution of study time and tutoring given good performance:\")\n",
    "print(result_2)\n",
    "\n",
    "result_3 = inference.query(variables=['StudyTime', 'Tutoring'], evidence={'Performance': 1})\n",
    "print(\"Probability distribution of study time and tutoring given average performance:\")\n",
    "print(result_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c580fb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n",
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n",
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n",
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n",
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n",
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n",
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of poor performance with medium study time and tutoring: 0.1\n",
      "Joint probability of high study time and yes tutoring given good performance: 0.1807228915662651\n",
      "Probability distribution for study time given average performance: [0.17857143 0.23809524 0.04761905]\n",
      "Probability distribution for tutoring given average performance: [0.14285714 0.29761905 0.0952381 ]\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Define the structure of the Bayesian Network\n",
    "model = BayesianModel([\n",
    "    ('studytime', 'performance'),\n",
    "    ('tutoring', 'performance')\n",
    "])\n",
    "\n",
    "# Define the Conditional Probability Distributions (CPDs)\n",
    "cpd1 = TabularCPD(variable='studytime', variable_card=3,\n",
    "                  values=[[0.3], [0.5], [0.2]],\n",
    "                  state_names={'studytime': ['low', 'medium', 'high']})\n",
    "\n",
    "cpd2 = TabularCPD(variable='tutoring', variable_card=2,\n",
    "                  values=[[0.5], [0.5]],\n",
    "                  state_names={'tutoring': ['yes', 'no']})\n",
    "\n",
    "cpd3 = TabularCPD(variable='performance', variable_card=3,\n",
    "                  values=[[0.2, 0.3, 0.1, 0.2, 0.05, 0.1],\n",
    "                          [0.5, 0.4, 0.4, 0.5, 0.2, 0.4],\n",
    "                          [0.3, 0.3, 0.5, 0.3, 0.75, 0.5]],\n",
    "                  evidence=['studytime', 'tutoring'],\n",
    "                  evidence_card=[3, 2],\n",
    "                  state_names={'performance': ['poor', 'average', 'good'],\n",
    "                               'studytime': ['low', 'medium', 'high'],\n",
    "                               'tutoring': ['yes', 'no']})\n",
    "\n",
    "model.add_cpds(cpd1, cpd2, cpd3)\n",
    "\n",
    "# Check if the model is valid\n",
    "assert model.check_model()\n",
    "\n",
    "# Initialize inference\n",
    "inference = VariableElimination(model)\n",
    "\n",
    "# Query for poor performance given medium study time and tutoring\n",
    "result_q1 = inference.query(variables=['performance'], evidence={'studytime': 'medium', 'tutoring': 'yes'})\n",
    "print(\"Probability of poor performance with medium study time and tutoring:\", result_q1.values[0])\n",
    "\n",
    "# Query for the distribution of study time and tutoring given good performance\n",
    "result_q2 = inference.query(variables=['studytime', 'tutoring'], evidence={'performance': 'good'})\n",
    "print(\"Joint probability of high study time and yes tutoring given good performance:\", result_q2.values[2, 0])\n",
    "\n",
    "# Query for the distribution of study time and tutoring given average performance\n",
    "result_q3 = inference.query(variables=['studytime', 'tutoring'], evidence={'performance': 'average'})\n",
    "print(\"Probability distribution for study time given average performance:\", result_q3.values[:, 0])\n",
    "print(\"Probability distribution for tutoring given average performance:\", result_q3.values[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e869a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define prior probabilities\n",
    "prior_weather = {'Sunny': 0.5, 'Rainy': 0.5}\n",
    "prior_umbrella_given_weather = {\n",
    "    'Sunny': {'Yes': 0.1, 'No': 0.9},\n",
    "    'Rainy': {'Yes': 0.8, 'No': 0.2}\n",
    "}\n",
    "\n",
    "# Define transition probabilities\n",
    "transition_weather = {\n",
    "    ('Sunny', 'Sunny'): 0.7,\n",
    "    ('Sunny', 'Rainy'): 0.3,\n",
    "    ('Rainy', 'Sunny'): 0.4,\n",
    "    ('Rainy', 'Rainy'): 0.6\n",
    "}\n",
    "\n",
    "# Define observation probabilities\n",
    "observation_umbrella_given_weather = {\n",
    "    'Sunny': {'Yes': 0.1, 'No': 0.9},\n",
    "    'Rainy': {'Yes': 0.8, 'No': 0.2}\n",
    "}\n",
    "\n",
    "def sample_weather_given_prior():\n",
    "    return np.random.choice(['Sunny', 'Rainy'], p=[prior_weather['Sunny'], prior_weather['Rainy']])\n",
    "\n",
    "def sample_umbrella_given_weather(weather):\n",
    "    return np.random.choice(['Yes', 'No'], p=[prior_umbrella_given_weather[weather]['Yes'], prior_umbrella_given_weather[weather]['No']])\n",
    "\n",
    "def sample_weather_given_previous(weather):\n",
    "    return np.random.choice(['Sunny', 'Rainy'], p=[transition_weather[(weather, 'Sunny')], transition_weather[(weather, 'Rainy')]])\n",
    "\n",
    "num_time_steps = 10\n",
    "current_weather = sample_weather_given_prior()\n",
    "for t in range(num_time_steps):\n",
    "    umbrella_decision = sample_umbrella_given_weather(current_weather)\n",
    "    print(f\"At time step {t}: Weather={current_weather}, Umbrella={umbrella_decision}\")\n",
    "    current_weather = sample_weather_given_previous(current_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03ac1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability that it is red given that it is blue   0.0\n",
      "blue probability  0.6666666666666666\n",
      "red probability  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "total_marbles=30\n",
    "red_marbles=10\n",
    "blue_marbles=20\n",
    "\n",
    "prob_red=red_marbles/total_marbles\n",
    "prob_blue=blue_marbles/total_marbles\n",
    "given=0\n",
    "\n",
    "#conditional formula P(R/B)=(P(B/R)*P(R)/P(B))\n",
    "prob=(given*prob_red)/prob_blue\n",
    "\n",
    "print(\"probability that it is red given that it is blue  \",prob)\n",
    "print(\"blue probability \",prob_blue)\n",
    "print(\"red probability \",prob_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995291d8",
   "metadata": {},
   "source": [
    "hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a423ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"healthy\", \"sick\"]\n",
    "n_states = len(states)\n",
    "print('number of hidden states ',n_states)\n",
    "observations = [\"cough\", \"no cough\"]\n",
    "n_observations = len(observations)\n",
    "print('number of observations ',n_observations)\n",
    "\n",
    "\n",
    "state_probability = np.array([0.4, 0.6])\n",
    "print(\"state probability: \", state_probability)\n",
    "\n",
    "transition_probability = np.array([[0.3, 0.7], [0.4, 0.6]])\n",
    "print(\"\\ntransition probability:\\n\", transition_probability)\n",
    "\n",
    "emission_probability= np.array([[0.9, 0.1], [0.2, 0.8]])\n",
    "print(\"\\nemission probability:\\n\", emission_probability)\n",
    "\n",
    "\n",
    "model = hmm.CategoricalHMM(n_components=n_states)\n",
    "model.startprob_ = state_probability\n",
    "model.transmat_ = transition_probability\n",
    "model.emissionprob_ = emission_probability\n",
    "\n",
    "observations_sequence = np.array([0, 1, 0, 1, 0, 0]).reshape(-1, 1)\n",
    "observations_sequence\n",
    "\n",
    "hidden_states = model.predict(observations_sequence)\n",
    "print(\"most likely hidden states \", hidden_states)\n",
    "\n",
    "log_probability, hidden_states = model.decode(observations_sequence, lengths =\n",
    "len(observations_sequence), algorithm ='viterbi' )\n",
    "print('log Probability :',log_probability)\n",
    "print(\"most likely hidden states:\", hidden_states)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(hidden_states, '-o', label=\"Hidden State\")\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Most Likely Hidden State')\n",
    "plt.title(\"Healthy or Sick\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19666988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plan 1\n",
    "P_P1 = 0.30\n",
    "#plan 2\n",
    "P_P2 = 0.20\n",
    "#plan 3\n",
    "P_P3 = 0.50\n",
    "\n",
    "#defective product plan 1\n",
    "P_D_P1 = 0.01 \n",
    "#defective product plan 2\n",
    "P_D_P2 = 0.03 \n",
    "#defective product plan 3\n",
    "P_D_P3 = 0.02 \n",
    "\n",
    "#total probability of a defective product\n",
    "P_D = P_P1 * P_D_P1 + P_P2 * P_D_P2 + P_P3 * P_D_P3\n",
    "\n",
    "#probabilities of plans\n",
    "P_P1_D = (P_D_P1 * P_P1) / P_D\n",
    "P_P2_D = (P_D_P2 * P_P2) / P_D\n",
    "P_P3_D = (P_D_P3 * P_P3) / P_D\n",
    "\n",
    "if P_P1_D > P_P2_D and P_P1_D > P_P3_D:\n",
    "    print(\"plan 1 was most likely used\")\n",
    "elif P_P2_D > P_P1_D and P_P2_D > P_P3_D:\n",
    "    print(\"plan 2 was most likely used\")\n",
    "else:\n",
    "    print(\"plan 3 was most likely used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc0eec",
   "metadata": {},
   "source": [
    "supervised learning regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a88b4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Price: [375000.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1500], [1800], [2200], [2500]]) \n",
    "y = np.array([300000, 350000, 400000, 450000])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "new_area = np.array([[2000]])\n",
    "predicted_price = model.predict(new_area)\n",
    "print(\"Predicted Price:\", predicted_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e99889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value (Polynomial Regression): [25.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([1, 4, 9, 16]) \n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_poly, y)\n",
    "\n",
    "X_new = np.array([[5]])\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "predicted_value = model_poly.predict(X_new_poly)\n",
    "print(\"Predicted Value (Polynomial Regression):\", predicted_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bb3fcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value (SVM Regression): [5.83333333]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([2, 3, 4, 5]) \n",
    "\n",
    "svm_regressor = SVR(kernel='linear')\n",
    "svm_regressor.fit(X, y)\n",
    "\n",
    "X_new = np.array([[5]])\n",
    "predicted_value = svm_regressor.predict(X_new)\n",
    "print(\"Predicted Value (SVM Regression):\", predicted_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adabbad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value (Decision Tree Regression): [5.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([2, 3, 4, 5]) \n",
    "\n",
    "tree_regressor = DecisionTreeRegressor()\n",
    "tree_regressor.fit(X, y)\n",
    "# Predict for new values\n",
    "X_new = np.array([[5]])\n",
    "predicted_value = tree_regressor.predict(X_new)\n",
    "print(\"Predicted Value (Decision Tree Regression):\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "596c9d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value (Random Forest Regression): [4.58]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([2, 3, 4, 5]) \n",
    "\n",
    "forest_regressor = RandomForestRegressor()\n",
    "forest_regressor.fit(X, y)\n",
    "\n",
    "X_new = np.array([[5]])\n",
    "predicted_value = forest_regressor.predict(X_new)\n",
    "print(\"Predicted Value (Random Forest Regression):\",predicted_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79495728",
   "metadata": {},
   "source": [
    "supervised learning classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00167635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Logistic Regression): 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy (Logistic Regression):\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b72c8d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (SVM): 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=42)\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy (SVM):\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d0a03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Decision Tree): 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy (Decision Tree):\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20ce60de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Random Forest): 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=42)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy (Random Forest):\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f234b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Accuracy (Naive Bayes): 1.0\n",
      "mse : 0.0\n",
      "rmse:  0.0\n",
      "r_squared (Naive Bayes): 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, confusion_matrix\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=42)\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"confusion matrix:\", conf_matrix)\n",
    "print(\"Accuracy (Naive Bayes):\", accuracy)\n",
    "print(\"mse :\", mse)\n",
    "print(\"rmse: \", rmse)\n",
    "print(\"r_squared (Naive Bayes):\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a461960b",
   "metadata": {},
   "source": [
    "unsupervised clustering learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930357ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60,random_state=0)\n",
    "\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=4)\n",
    "labels = agg_clustering.fit_predict(X)\n",
    "\n",
    "silhouette = silhouette_score(X, labels)\n",
    "calinski_harabasz = calinski_harabasz_score(X, labels)\n",
    "davies_bouldin = davies_bouldin_score(X, labels)\n",
    "print(silhouette)\n",
    "print(calinski_harabasz)\n",
    "print(davies_bouldin)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.title(\"Hierarchical Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200,\n",
    "c='red', marker='X', label='Centroids')\n",
    "plt.title(\"K-means Clustering\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f06c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "features = [\"feature1\", \"feature2\", ...]\n",
    "X = data[features]\n",
    "\n",
    "wcss = []\n",
    "for k in range(1, 11):\n",
    "  kmeans = KMeans(n_clusters=k, init='k-means++')\n",
    "  kmeans.fit(X)\n",
    "  wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Within-cluster sum of squares\")\n",
    "plt.show()\n",
    "\n",
    "k = 3\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++')\n",
    "kmeans.fit(X)\n",
    "\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "data['cluster'] = cluster_labels\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "plt.scatter(X['feature1'], X['feature2'], c=cluster_labels)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"K-means Clustering Results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3665bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "pca=PCA(n_components=2)\n",
    "X_pca=pca.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap='viridis')\n",
    "plt.title(\"PCA of wine dataset\")\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a17072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "silhouette = silhouette_score(X, labels)\n",
    "calinski_harabasz = calinski_harabasz_score(X, labels)\n",
    "davies_bouldin = davies_bouldin_score(X, labels)\n",
    "adjusted_rand = adjusted_rand_score(y, labels) \n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X', label='Centroid')\n",
    "plt.title(\"Clustering of Iris Data\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "silhouette, calinski_harabasz, davies_bouldin, adjusted_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2624c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.title(\"PCA Visualization of Iris Dataset\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fde8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "gmm = GaussianMixture(n_components=4)\n",
    "labels = gmm.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.title(\"Gaussian Mixture Model (GMM) Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c851cc",
   "metadata": {},
   "source": [
    "unsupervised assosciatian learning models\n",
    "fp growth and aprori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4529d7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   support         itemsets\n",
      "0      0.8          (bread)\n",
      "1      0.6         (butter)\n",
      "2      0.8           (milk)\n",
      "3      0.4  (bread, butter)\n",
      "4      0.6    (bread, milk)\n",
      "5      0.4   (butter, milk)\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import pandas as pd\n",
    "\n",
    "dataset = [['bread', 'milk', 'eggs'],\n",
    "['bread', 'butter'],\n",
    "['milk', 'butter'],\n",
    "['bread', 'milk', 'butter'],\n",
    "['bread', 'milk']]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets = apriori(df, min_support=0.4, use_colnames=True)\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91e1b1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   support         itemsets\n",
      "0      0.8           (milk)\n",
      "1      0.8          (bread)\n",
      "2      0.6         (butter)\n",
      "3      0.6    (bread, milk)\n",
      "4      0.4  (bread, butter)\n",
      "5      0.4   (butter, milk)\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import pandas as pd\n",
    "\n",
    "dataset = [['bread', 'milk', 'eggs'],\n",
    "['bread', 'butter'],\n",
    "['milk', 'butter'],\n",
    "['bread', 'milk', 'butter'],\n",
    "['bread', 'milk']]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets = fpgrowth(df, min_support=0.4, use_colnames=True)\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0df01a",
   "metadata": {},
   "source": [
    "gridsearch cv task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6096ff91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "default model accuracy  1.0\n",
      "best model accuracy  1.0\n",
      "best model parameters  {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "default_model = SVC(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=SVC(random_state=42), param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_default = default_model.fit(X_train, y_train).predict(X_test)\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "\n",
    "print(\"default model accuracy \", accuracy_default)\n",
    "print(\"best model accuracy \", accuracy_best)\n",
    "print(\"best model parameters \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82544ae",
   "metadata": {},
   "source": [
    "essemble learning comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "566c4323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest regressor root mean squared  55.06221962777271\n",
      "random forest regressor co-efficient of determination  0.4277536442428346\n",
      "indivual base mode root mean squared  69.93865690973111\n",
      "indivual base mode co-efficient of determination  69.93865690973111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "X_tain, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "\n",
    "regressor = RandomForestRegressor()\n",
    "\n",
    "regressor.fit(X_tain,y_train)\n",
    "\n",
    "y_pred= regressor.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"random forest regressor root mean squared \", rmse)\n",
    "print(\"random forest regressor co-efficient of determination \", r2)\n",
    "\n",
    "#for indivual base using decsion tree\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_tain,y_train)\n",
    "y_pred=tree.predict(X_test)\n",
    "\n",
    "rmse_tree = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "r2_tree = r2_score(y_test,y_pred)\n",
    "\n",
    "print(\"indivual base mode root mean squared \",rmse_tree)\n",
    "print(\"indivual base mode co-efficient of determination \",rmse_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af992e5a",
   "metadata": {},
   "source": [
    "reinforcement q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b694ff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\mujta\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: gym in c:\\users\\mujta\\anaconda3\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\mujta\\anaconda3\\lib\\site-packages (from gym) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mujta\\anaconda3\\lib\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\mujta\\anaconda3\\lib\\site-packages (from gym) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e31dc6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d329868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77e96f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8bb3a96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24cdd3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 20000       # Total episodes\n",
    "learning_rate = 0.7          # Learning rate\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.7                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability\n",
    "decay_rate = 0.5            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04226984",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "\n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "            #print(exp_exp_tradeoff, \"action\", action)\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            #print(\"action random\", action)\n",
    "\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "\n",
    "        total_rewards += reward\n",
    "\n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "\n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01cf6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            #env.render()\n",
    "            if new_state == 15:\n",
    "                print(\"We reached our Goal üèÜ\")\n",
    "            else:\n",
    "                print(\"We fell into a hole ‚ò†Ô∏è\")\n",
    "\n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "env = gym.make('FrozenLake-v1')\n",
    "print(\"Action space:\", env.action_space)\n",
    "#print(\"State space:\", env.env.nS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2082488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "571b80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 16\n",
    "action_space = env.action_space.n\n",
    "alpha = 0.5\n",
    "gamma = 0.7\n",
    "state_action_vals = np.random.randn(state_size, action_space)\n",
    "policy = np.zeros(state_size, dtype=int)\n",
    "episodes = 20000\n",
    "eps = 1\n",
    "test_episodes = 50\n",
    "test_every = 1000\n",
    "test_episode = []\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "741cb13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, eps):\n",
    "    sample = np.random.uniform()\n",
    "    if sample < eps:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return state_action_vals[state].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    action = select_action(state, eps)\n",
    "    done = False\n",
    "    while not done:\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_action = select_action(state, eps)\n",
    "\n",
    "        action_value = state_action_vals[state, action]\n",
    "        next_action_value = state_action_vals[next_state, next_action]\n",
    "        delta = reward + gamma * next_action_value - action_value\n",
    "        state_action_vals[state, action] += alpha * delta\n",
    "        state, action = next_state, next_action\n",
    "\n",
    "    if ep % test_every == 0:\n",
    "        total_rewards = 0\n",
    "        for _ in range(test_episodes):\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "            while not done:\n",
    "                action = state_action_vals[state].argmax()\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                total_rewards += reward\n",
    "        rewards.append(total_rewards / test_episodes)\n",
    "        test_episode.append(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172e7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_episode, rewards)\n",
    "ax.set_title('Episodes vs average rewards')\n",
    "ax.set_xlabel('Episode')\n",
    "_ = ax.set_ylabel('Average reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd7ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Parameters for Q-learning\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 200\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "exploration_rate = 1.0\n",
    "max_exploration_rate = 1.0\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.01\n",
    "\n",
    "# Discretization parameters for state space\n",
    "num_buckets = (1, 1, 6, 12)  # Number of buckets for each feature in state space\n",
    "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "state_bounds[1] = [-0.5, 0.5]\n",
    "state_bounds[3] = [-np.radians(50), np.radians(50)]\n",
    "\n",
    "# Initialize Q-table\n",
    "action_space_size = env.action_space.n\n",
    "q_table = np.zeros(num_buckets + (action_space_size,))\n",
    "\n",
    "# Function to discretize the continuous state into discrete buckets\n",
    "def discretize_state(state):\n",
    "    discretized_state = []\n",
    "    for i in range(len(state)):\n",
    "        scaling = (state[i] + abs(state_bounds[i][0])) / (state_bounds[i][1] - state_bounds[i][0])\n",
    "        bucket = int(round((num_buckets[i] - 1) * scaling))\n",
    "        discretized_state.append(min(num_buckets[i] - 1, max(0, bucket)))\n",
    "    return tuple(discretized_state)\n",
    "\n",
    "# List to store rewards\n",
    "rewards_all_episodes = []\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = discretize_state(state)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = np.random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take action and observe next state and reward\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_state = discretize_state(new_state)\n",
    "\n",
    "        # Update Q-table\n",
    "        best_next_action = np.argmax(q_table[new_state])\n",
    "        q_table[state + (action,)] += learning_rate * (reward + discount_rate * q_table[new_state + (best_next_action,)] - q_table[state + (action,)])\n",
    "\n",
    "        # Transition to next state\n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Append total reward of current episode\n",
    "    rewards_all_episodes.append(total_reward)\n",
    "\n",
    "    # Decay exploration rate\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "                        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "\n",
    "    # Print episode information\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}: Total Reward = {total_reward}\")\n",
    "\n",
    "# Plotting rewards over episodes\n",
    "plt.plot(np.arange(1, num_episodes + 1), rewards_all_episodes)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Rewards over Episodes')\n",
    "plt.show()\n",
    "\n",
    "# Test the trained agent\n",
    "state = env.reset()\n",
    "state = discretize_state(state)\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    new_state = discretize_state(new_state)\n",
    "    total_reward += reward\n",
    "    state = new_state\n",
    "\n",
    "print(f\"Total reward for test episode: {total_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ecf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the Policy Network using a simple feedforward neural network\n",
    "class PolicyNetwork(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = Dense(hidden_size, activation='relu')\n",
    "        self.fc2 = Dense(action_size, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to choose action based on policy probabilities\n",
    "def choose_action(state, policy_network):\n",
    "    state = np.expand_dims(state, axis=0)  # Add batch dimension\n",
    "    logits = policy_network(state)\n",
    "\n",
    "    # Apply numerical stability adjustment\n",
    "    epsilon = 1e-6\n",
    "    logits = logits - np.max(logits)  # Subtract maximum value for numerical stability\n",
    "    action_probs = np.exp(logits) / (np.sum(np.exp(logits)) + epsilon)  # Softmax with epsilon\n",
    "\n",
    "    action = np.random.choice(len(action_probs[0]), p=action_probs[0])\n",
    "    return action\n",
    "\n",
    "# Function to compute discounted returns\n",
    "def compute_discounted_returns(rewards, discount_factor):\n",
    "    discounted_returns = np.zeros_like(rewards)\n",
    "    cumulative_return = 0.0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        cumulative_return = rewards[t] + discount_factor * cumulative_return\n",
    "        discounted_returns[t] = cumulative_return\n",
    "    return discounted_returns\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "discount_factor = 0.99\n",
    "num_episodes = 1000\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initialize Policy Network\n",
    "policy_network = PolicyNetwork(state_size, action_size, 32)\n",
    "optimizer = Adam(learning_rate)\n",
    "\n",
    "# REINFORCE Training Loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_states, episode_actions, episode_rewards = [], [], []\n",
    "\n",
    "    while True:\n",
    "        action = choose_action(state, policy_network)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        episode_states.append(state)\n",
    "        episode_actions.append(action)\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # Compute discounted returns\n",
    "            discounted_returns = compute_discounted_returns(episode_rewards, discount_factor)\n",
    "\n",
    "            # Convert lists to numpy arrays\n",
    "            episode_states = np.array(episode_states)\n",
    "            episode_actions = np.array(episode_actions)\n",
    "            discounted_returns = np.array(discounted_returns)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Compute log probabilities of selected actions\n",
    "                logits = policy_network(episode_states)\n",
    "                action_masks = tf.one_hot(episode_actions, action_size)\n",
    "                log_probs = tf.reduce_sum(action_masks * tf.math.log(logits), axis=1)\n",
    "\n",
    "                # Compute policy loss\n",
    "                policy_loss = -tf.reduce_mean(log_probs * discounted_returns)\n",
    "\n",
    "            # Update policy network\n",
    "            gradients = tape.gradient(policy_loss, policy_network.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, policy_network.trainable_variables))\n",
    "\n",
    "            break\n",
    "\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f\"Episode {episode + 1}: Average Reward = {sum(episode_rewards)}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "445b4c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Environment settings\n",
    "grid_size = 5\n",
    "goal = (4, 4)\n",
    "obstacles = [(1, 1), (2, 2), (3, 3)]\n",
    "actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]  # Right, Down, Left, Up\n",
    "\n",
    "# Q-learning settings\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "episodes = 1000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros((grid_size, grid_size, len(actions)))\n",
    "\n",
    "def is_terminal_state(state):\n",
    "    return state == goal or state in obstacles\n",
    "\n",
    "def get_next_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return np.argmax(Q[state[0], state[1]])\n",
    "    else:\n",
    "        return random.randint(0, len(actions) - 1)\n",
    "\n",
    "def get_next_location(state, action_index):\n",
    "    new_state = (state[0] + actions[action_index][0], state[1] + actions[action_index][1])\n",
    "    if new_state[0] >= 0 and new_state[0] < grid_size and new_state[1] >= 0 and new_state[1] < grid_size:\n",
    "        return new_state\n",
    "    return state  # Return to same state if out of bounds\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = (random.randint(0, grid_size-1), random.randint(0, grid_size-1))\n",
    "    if state in obstacles:\n",
    "        continue\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        action_index = get_next_action(state, epsilon)\n",
    "        next_state = get_next_location(state, action_index)\n",
    "        reward = 1 if next_state == goal else -1 if next_state in obstacles else 0\n",
    "        old_value = Q[state[0], state[1], action_index]\n",
    "        future_optimal_value = np.max(Q[next_state[0], next_state[1]])\n",
    "        Q[state[0], state[1], action_index] = old_value + alpha * (reward + gamma * future_optimal_value - old_value)\n",
    "        \n",
    "        if next_state == goal or next_state in obstacles:\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "314750d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí ‚Üí ‚Üí ‚Üí ‚Üì\n",
      "‚Üë ‚õî ‚Üí ‚Üí ‚Üì\n",
      "‚Üì ‚Üì ‚õî ‚Üí ‚Üì\n",
      "‚Üì ‚Üì ‚Üì ‚õî ‚Üì\n",
      "‚Üí ‚Üí ‚Üí ‚Üí üèÅ\n",
      "Path to goal: [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 4), (2, 4), (3, 4), (4, 4)]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_policy(grid_size, Q):\n",
    "    direction_map = {0: '‚Üí', 1: '‚Üì', 2: '‚Üê', 3: '‚Üë'}\n",
    "    policy = [[' ' for _ in range(grid_size)] for _ in range(grid_size)]\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            if (i, j) in obstacles:\n",
    "                policy[i][j] = '‚õî'\n",
    "            elif (i, j) == goal:\n",
    "                policy[i][j] = 'üèÅ'\n",
    "            else:\n",
    "                best_action = np.argmax(Q[i, j])\n",
    "                policy[i][j] = direction_map[best_action]\n",
    "    for row in policy:\n",
    "        print(' '.join(row))\n",
    "\n",
    "print_policy(grid_size, Q)\n",
    "\n",
    "\n",
    "def test_agent(Q, start, goal):\n",
    "    state = start\n",
    "    path = [state]\n",
    "    while state != goal:\n",
    "        action_index = np.argmax(Q[state[0], state[1]])\n",
    "        state = get_next_location(state, action_index)\n",
    "        path.append(state)\n",
    "        if state in obstacles:\n",
    "            print(\"Hit an obstacle!\")\n",
    "            return\n",
    "        if len(path) > grid_size * grid_size:  # safeguard against infinite loops\n",
    "            print(\"Failed to reach goal.\")\n",
    "            return\n",
    "    print(\"Path to goal:\", path)\n",
    "\n",
    "# Example test\n",
    "test_agent(Q, (0, 0), goal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06e44c77",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_episodes):\n\u001b[0;32m     37\u001b[0m     initial_observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 38\u001b[0m     current_state \u001b[38;5;241m=\u001b[39m discretize_state(initial_observation)\n\u001b[0;32m     39\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m get_alpha(episode)\n\u001b[0;32m     40\u001b[0m     epsilon \u001b[38;5;241m=\u001b[39m get_epsilon(episode)\n",
      "Cell \u001b[1;32mIn[64], line 14\u001b[0m, in \u001b[0;36mdiscretize_state\u001b[1;34m(observation, bins, bounds)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdiscretize_state\u001b[39m(observation, bins\u001b[38;5;241m=\u001b[39mn_bins, bounds\u001b[38;5;241m=\u001b[39mlower_bounds \u001b[38;5;241m+\u001b[39m upper_bounds):\n\u001b[1;32m---> 14\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([observation[\u001b[38;5;241m2\u001b[39m], observation[\u001b[38;5;241m3\u001b[39m]])  \u001b[38;5;66;03m# using only angle and angular velocity\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     digitized \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mdigitize(state[i], np\u001b[38;5;241m.\u001b[39mlinspace(bounds[\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mi], bounds[\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], bins[i]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)]\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([x\u001b[38;5;241m*\u001b[39m(bins[i]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(digitized)])\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Define the discretization\n",
    "n_bins = (6, 12)\n",
    "lower_bounds = [env.observation_space.low[2], -0.5]  # bounds for pole angle and pole velocity\n",
    "upper_bounds = [env.observation_space.high[2], 0.5]  # bounds for pole angle and pole velocity\n",
    "\n",
    "# Discretize the continuous state into bins\n",
    "def discretize_state(observation, bins=n_bins, bounds=lower_bounds + upper_bounds):\n",
    "    state = np.array([observation[2], observation[3]])  # using only angle and angular velocity\n",
    "    digitized = [int(np.digitize(state[i], np.linspace(bounds[2*i], bounds[2*i+1], bins[i]-1))) for i in range(2)]\n",
    "    return sum([x*(bins[i]**i) for i, x in enumerate(digitized)])\n",
    "\n",
    "# Q-learning settings\n",
    "n_episodes = 1000\n",
    "min_alpha = 0.1  # Learning rate\n",
    "min_epsilon = 0.1  # Exploration rate\n",
    "gamma = 1.0  # Discount factor\n",
    "ada_divisor = 25  # For adaptive learning rate and epsilon\n",
    "\n",
    "# Initialize Q table\n",
    "Q = np.zeros(n_bins + (env.action_space.n,))\n",
    "\n",
    "# Adaptive learning rate and exploration rate\n",
    "def get_alpha(t):\n",
    "    return max(min_alpha, min(1.0, 1.0 - np.log10((t + 1) / ada_divisor)))\n",
    "\n",
    "def get_epsilon(t):\n",
    "    return max(min_epsilon, min(1, 1.0 - np.log10((t + 1) / ada_divisor)))\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(n_episodes):\n",
    "    initial_observation = env.reset()\n",
    "    current_state = discretize_state(initial_observation)\n",
    "    alpha = get_alpha(episode)\n",
    "    epsilon = get_epsilon(episode)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q[current_state])  # Exploit\n",
    "\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        new_state = discretize_state(obs)\n",
    "        Q[current_state][action] += alpha * (reward + gamma * np.max(Q[new_state]) - Q[current_state][action])\n",
    "        current_state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c0ded094",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Inside the for loop where episodes are run:\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_episodes):\n\u001b[1;32m----> 7\u001b[0m     current_state \u001b[38;5;241m=\u001b[39m discretize_state(env\u001b[38;5;241m.\u001b[39mreset())\n\u001b[0;32m      8\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m get_alpha(episode)\n\u001b[0;32m      9\u001b[0m     epsilon \u001b[38;5;241m=\u001b[39m get_epsilon(episode)\n",
      "Cell \u001b[1;32mIn[58], line 15\u001b[0m, in \u001b[0;36mdiscretize_state\u001b[1;34m(observation, bins, bounds)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdiscretize_state\u001b[39m(observation, bins\u001b[38;5;241m=\u001b[39mn_bins, bounds\u001b[38;5;241m=\u001b[39mlower_bounds \u001b[38;5;241m+\u001b[39m upper_bounds):\n\u001b[1;32m---> 15\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([observation[\u001b[38;5;241m2\u001b[39m], observation[\u001b[38;5;241m3\u001b[39m]])  \u001b[38;5;66;03m# using only angle and angular velocity\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     digitized \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mdigitize(state[i], np\u001b[38;5;241m.\u001b[39mlinspace(bounds[\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mi], bounds[\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], bins[i]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)]\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([x\u001b[38;5;241m*\u001b[39m(bins[i]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(digitized)])\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episode_lengths = []\n",
    "\n",
    "# Inside the for loop where episodes are run:\n",
    "for episode in range(n_episodes):\n",
    "    current_state = discretize_state(env.reset())\n",
    "    alpha = get_alpha(episode)\n",
    "    epsilon = get_epsilon(episode)\n",
    "    done = False\n",
    "    steps = 0  # Track the number of steps in this episode\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q[current_state])  # Exploit\n",
    "\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        new_state = discretize_state(obs)\n",
    "        Q[current_state][action] += alpha * (reward + gamma * np.max(Q[new_state]) - Q[current_state][action])\n",
    "        current_state = new_state\n",
    "        steps += 1\n",
    "    \n",
    "    episode_lengths.append(steps)\n",
    "\n",
    "# After the for loop, plot the results\n",
    "plt.plot(episode_lengths)\n",
    "plt.title('Episode Lengths Over Time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "05a35e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player X wins\n",
      "[['X' 'X' 'X']\n",
      " [' ' 'O' 'O']\n",
      " [' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.full((3, 3), ' ', dtype=str)  # Use a character array\n",
    "\n",
    "        \n",
    "    def valid_moves(self):\n",
    "        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == ' ']\n",
    "\n",
    "    def make_move(self, move, player):\n",
    "        if self.board[move] == ' ':\n",
    "            self.board[move] = player\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_winner(self, player):\n",
    "        for i in range(3):\n",
    "            if all(self.board[i, :] == player) or all(self.board[:, i] == player):\n",
    "                return True\n",
    "        if self.board[0, 0] == self.board[1, 1] == self.board[2, 2] == player or \\\n",
    "           self.board[0, 2] == self.board[1, 1] == self.board[2, 0] == player:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_full(self):\n",
    "        return not any(self.board[i, j] == ' ' for i in range(3) for j in range(3))\n",
    "\n",
    "def train_agent(episodes=10000):\n",
    "    Q = {}\n",
    "    learning_rate = 0.1\n",
    "    discount_factor = 0.9\n",
    "    epsilon = 0.2\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        game = TicTacToe()\n",
    "        player = 'X'\n",
    "        state = tuple(map(tuple, game.board))\n",
    "        Q.setdefault(state, np.zeros(9))\n",
    "\n",
    "        while True:\n",
    "            moves = game.valid_moves()\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice(moves)\n",
    "            else:\n",
    "                q_values = Q[state]\n",
    "                action = moves[np.argmax(q_values[np.array([m[0]*3 + m[1] for m in moves])])]\n",
    "\n",
    "            game.make_move(action, player)\n",
    "            next_state = tuple(map(tuple, game.board))\n",
    "            Q.setdefault(next_state, np.zeros(9))\n",
    "            \n",
    "            reward = 0\n",
    "            if game.check_winner(player):\n",
    "                reward = 1\n",
    "            elif game.is_full():\n",
    "                reward = 0.5\n",
    "\n",
    "            Q[state][3*action[0]+action[1]] += learning_rate * (reward + discount_factor * np.max(Q[next_state]) - Q[state][3*action[0]+action[1]])\n",
    "\n",
    "            if reward > 0 or game.is_full():\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            player = 'O' if player == 'X' else 'X'\n",
    "\n",
    "    return Q\n",
    "\n",
    "def play_game(Q):\n",
    "    game = TicTacToe()\n",
    "    player = 'X'\n",
    "    while True:\n",
    "        state = tuple(map(tuple, game.board))\n",
    "        if player == 'X':\n",
    "            q_values = Q[state]\n",
    "            action = tuple(np.divmod(np.argmax(q_values), 3))\n",
    "        else:\n",
    "            action = random.choice(game.valid_moves())\n",
    "\n",
    "        game.make_move(action, player)\n",
    "        if game.check_winner(player):\n",
    "            print(f\"player {player} wins\")\n",
    "            break\n",
    "        if game.is_full():\n",
    "            print(\"it's a draw!\")\n",
    "            break\n",
    "        player = 'O' if player == 'X' else 'X'\n",
    "\n",
    "    print(game.board)\n",
    "\n",
    "#train the q-learning agent\n",
    "Q = train_agent()\n",
    "\n",
    "#testing the trained agent\n",
    "play_game(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e82eac37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting joint angles: [1.25663706 1.88495559]\n",
      "Step: 1 Joint angles: [0.62831853 1.25663706] Reward: -39.569660112501055\n",
      "Step: 2 Joint angles: [-0.62831853  0.        ] Reward: -33.564338553438\n",
      "Step: 3 Joint angles: [-1.88495559 -0.62831853] Reward: -45.75\n",
      "Step: 4 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 5 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 6 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 7 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 8 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 9 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 10 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 11 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 12 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 13 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 14 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 15 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 16 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 17 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 18 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 19 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 20 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 21 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 22 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 23 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 24 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 25 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 26 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 27 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 28 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 29 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 30 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 31 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 32 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 33 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 34 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 35 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 36 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 37 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 38 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 39 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 40 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 41 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 42 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 43 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 44 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 45 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 46 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 47 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 48 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 49 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Step: 50 Joint angles: [-1.25663706  0.        ] Reward: -38.00532155906305\n",
      "Final joint angles: [-1.25663706  0.        ]\n",
      "Distance to target position: 6.164845623295286\n",
      "Total reward accumulated: -1905.1341119419035\n"
     ]
    }
   ],
   "source": [
    "#robot arm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "n_joints = 2  \n",
    "joint_angle_limits = (-np.pi, np.pi) \n",
    "n_actions = 5  \n",
    "n_states_per_joint = 10 \n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.95\n",
    "n_episodes = 1000\n",
    "max_steps_per_episode = 50\n",
    "epsilon = 0.2\n",
    "\n",
    "target_position = np.array([5, 5])\n",
    "\n",
    "q_table = np.zeros((100, 25))\n",
    "\n",
    "def get_state_from_angles(angles):\n",
    "    bins = np.linspace(*joint_angle_limits, n_states_per_joint + 1)\n",
    "    discretized = np.digitize(angles, bins=bins, right=False) - 1\n",
    "    discretized = np.clip(discretized, 0, n_states_per_joint - 1)\n",
    "    state = np.ravel_multi_index(discretized, (n_states_per_joint,)*n_joints)\n",
    "    return state\n",
    "\n",
    "def get_angles_from_state(state):\n",
    "    indexes = np.unravel_index(state, (n_states_per_joint,)*n_joints)\n",
    "    angle_values = np.linspace(*joint_angle_limits, n_states_per_joint+1)\n",
    "    angles = np.array([angle_values[i] for i in indexes])\n",
    "    return angles\n",
    "\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, n_actions**n_joints - 1)\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "def simulate_robot(angles, action):\n",
    "    action_combinations = np.unravel_index(action, (n_actions,)*n_joints)\n",
    "    new_angles = angles + (np.array(action_combinations) - n_actions//2) * (2*np.pi / n_states_per_joint)\n",
    "    new_angles = np.clip(new_angles, *joint_angle_limits)  \n",
    "   \n",
    "    reward = -np.sum((target_position - np.cos(new_angles))**2)\n",
    "    return new_angles, reward\n",
    "\n",
    "# training loop\n",
    "for episode in range(n_episodes):\n",
    "    state_index = random.randint(0, 99)\n",
    "    angles = get_angles_from_state(state_index)  \n",
    "\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        action = choose_action(state_index)\n",
    "        new_angles, reward = simulate_robot(angles, action)\n",
    "        new_state_index = get_state_from_angles(new_angles)\n",
    "        best_future_q = np.max(q_table[new_state_index])\n",
    "        q_table[state_index, action] = (1 - learning_rate) * q_table[state_index, action] + \\\n",
    "                                       learning_rate * (reward + discount_factor * best_future_q)\n",
    "        state_index = new_state_index\n",
    "        angles = new_angles\n",
    "        \n",
    "def print_output():\n",
    "    state_index = random.randint(0, 99)\n",
    "    angles = get_angles_from_state(state_index)\n",
    "    state = get_state_from_angles(angles)\n",
    "    total_reward = 0\n",
    "    print(\"Starting joint angles:\", angles)\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        action = np.argmax(q_table[state]) \n",
    "        angles, reward = simulate_robot(angles, action)  \n",
    "        state = get_state_from_angles(angles)  \n",
    "        total_reward += reward  \n",
    "        print(\"Step:\", step + 1, \"Joint angles:\", angles, \"Reward:\", reward)\n",
    "\n",
    "    distance_to_target = np.sqrt(np.sum((target_position - np.cos(angles))**2))\n",
    "    print(\"Final joint angles:\", angles)\n",
    "    print(\"Distance to target position:\", distance_to_target)\n",
    "    print(\"Total reward accumulated:\", total_reward)\n",
    "\n",
    "print_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f20a45e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Environment setup\u001b[39;00m\n\u001b[0;32m      9\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPong-v0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make('Pong-v0')\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((50, 50)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Hyperparameters\n",
    "HIDDEN_SIZE = 200\n",
    "LEARNING_RATE = 0.0005\n",
    "EPISODES = 1000\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Model definition\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 * 9 * 9, HIDDEN_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_SIZE, env.action_space.n),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# REINFORCE function\n",
    "def reinforce(policy_network, optimizer, rewards, log_probs):\n",
    "    discounted_rewards = []\n",
    "    cumulative_rewards = 0\n",
    "    for reward in reversed(rewards):\n",
    "        cumulative_rewards = reward + cumulative_rewards * GAMMA\n",
    "        discounted_rewards.insert(0, cumulative_rewards)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "\n",
    "    policy_gradient = []\n",
    "    for log_prob, reward in zip(log_probs, discounted_rewards):\n",
    "        policy_gradient.append(-log_prob * reward)\n",
    "    policy_gradient = torch.stack(policy_gradient).sum()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_gradient.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Main training loop\n",
    "policy_net = PolicyNetwork()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = preprocess(state).unsqueeze(0)\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        probabilities = policy_net(state)\n",
    "        action = np.random.choice(env.action_space.n, p=np.array(probabilities.detach().squeeze()))\n",
    "        log_prob = torch.log(probabilities.squeeze(0)[action])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = preprocess(next_state).unsqueeze(0)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        state = next_state\n",
    "\n",
    "    reinforce(policy_net, optimizer, rewards, log_probs)\n",
    "    print(f\"Episode {episode + 1}: Total Reward: {sum(rewards)}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "776d9ed5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 2 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 61\u001b[0m\n\u001b[0;32m     57\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Convert state to a 1x3 tensor\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([state], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     62\u001b[0m     mu, sigma \u001b[38;5;241m=\u001b[39m policy_net(state_tensor)\n\u001b[0;32m     63\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Normal(mu, sigma)\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 2 (got 0)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# Hyperparameters\n",
    "HIDDEN_SIZE = 128\n",
    "LEARNING_RATE = 0.003\n",
    "EPISODES = 200\n",
    "GAMMA = 0.9\n",
    "\n",
    "# Model definition\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, HIDDEN_SIZE)\n",
    "        self.fc2 = nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        self.mu_layer = nn.Linear(HIDDEN_SIZE, 1)\n",
    "        self.sigma_layer = nn.Linear(HIDDEN_SIZE, 1)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mu = 2 * self.activation(self.mu_layer(x))\n",
    "        sigma = torch.exp(self.sigma_layer(x)) + 1e-5\n",
    "        return mu, sigma\n",
    "\n",
    "def reinforce(policy_network, optimizer, rewards, log_probs):\n",
    "    discounted_rewards = []\n",
    "    cumulative_rewards = 0\n",
    "    for reward in reversed(rewards):\n",
    "        cumulative_rewards = reward + cumulative_rewards * GAMMA\n",
    "        discounted_rewards.insert(0, cumulative_rewards)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "\n",
    "    policy_loss = []\n",
    "    for log_prob, reward in zip(log_probs, discounted_rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "policy_net = PolicyNetwork()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Convert state to a 1x3 tensor\n",
    "        state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "        mu, sigma = policy_net(state_tensor)\n",
    "        dist = Normal(mu, sigma)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        next_state, reward, done, _ = env.step([action.item()])\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        state = next_state\n",
    "\n",
    "    reinforce(policy_net, optimizer, rewards, log_probs)\n",
    "    print(f\"Episode {episode + 1}: Total Reward: {sum(rewards)}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac182f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create arrays\n",
    "arr1 = np.array([1, 2, 3, 4])\n",
    "arr2 = np.array([5, 6, 7, 8])\n",
    "arr3 = np.array([9, 10, 11, 12])\n",
    "\n",
    "# Display arrays\n",
    "print(\"Array 1:\", arr1)\n",
    "print(\"Array 2:\", arr2)\n",
    "print(\"Array 3:\", arr3)\n",
    "\n",
    "# Operations\n",
    "print(\"\\nOperations:\")\n",
    "\n",
    "# Addition\n",
    "print(\"Addition of arr1 and arr2:\", np.add(arr1, arr2))\n",
    "\n",
    "# Subtraction\n",
    "print(\"Subtraction of arr1 from arr2:\", np.subtract(arr2, arr1))\n",
    "\n",
    "# Multiplication\n",
    "print(\"Element-wise multiplication of arr1 and arr2:\", np.multiply(arr1, arr2))\n",
    "\n",
    "# Division\n",
    "print(\"Element-wise division of arr2 by arr1:\", np.divide(arr2, arr1))\n",
    "\n",
    "# Square root\n",
    "print(\"Square root of arr3:\", np.sqrt(arr3))\n",
    "\n",
    "# Exponential\n",
    "print(\"Exponential of arr1:\", np.exp(arr1))\n",
    "\n",
    "# Dot product\n",
    "print(\"Dot product of arr1 and arr2:\", np.dot(arr1, arr2))\n",
    "\n",
    "# Transpose\n",
    "arr4 = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"Transpose of arr4:\\n\", arr4.T)\n",
    "\n",
    "# Sum\n",
    "print(\"Sum of arr3 elements:\", np.sum(arr3))\n",
    "\n",
    "# Mean\n",
    "print(\"Mean of arr1:\", np.mean(arr1))\n",
    "\n",
    "# Max\n",
    "print(\"Maximum element of arr2:\", np.max(arr2))\n",
    "\n",
    "# Min\n",
    "print(\"Minimum element of arr3:\", np.min(arr3))\n",
    "\n",
    "# Sorting\n",
    "print(\"Sorted arr1:\", np.sort(arr1))\n",
    "\n",
    "print(\"Concatenation of arr1 and arr2:\", np.concatenate((arr1, arr2)))\n",
    "\n",
    "# Reshaping\n",
    "print(\"Reshaped arr1 into 2x2 array:\\n\", arr1.reshape(2, 2))\n",
    "\n",
    "# Element-wise power\n",
    "print(\"Element-wise power of arr1 (squared):\", np.power(arr1, 2))\n",
    "\n",
    "# Absolute value\n",
    "print(\"Absolute value of arr2:\", np.abs(arr2))\n",
    "\n",
    "# Trigonometric functions (sine)\n",
    "print(\"Sine of arr1:\", np.sin(arr1))\n",
    "\n",
    "# Element-wise comparison\n",
    "print(\"Element-wise comparison (arr2 > arr1):\", arr2 > arr1)\n",
    "\n",
    "# Element-wise logical operations\n",
    "print(\"Element-wise logical AND between arr1 and arr2:\", np.logical_and(arr1, arr2))\n",
    "print(\"Element-wise logical OR between arr1 and arr2:\", np.logical_or(arr1, arr2))\n",
    "print(\"Element-wise logical NOT of arr1:\", np.logical_not(arr1))\n",
    "\n",
    "# Cumulative sum\n",
    "print(\"Cumulative sum of arr3:\", np.cumsum(arr3))\n",
    "\n",
    "# Unique values\n",
    "arr4 = np.array([1, 2, 2, 3, 3, 3])\n",
    "print(\"Unique elements of arr4:\", np.unique(arr4))\n",
    "\n",
    "# Reversing\n",
    "print(\"Reversed arr1:\", np.flip(arr1))\n",
    "\n",
    "# Index of maximum and minimum values\n",
    "print(\"Index of maximum value in arr2:\", np.argmax(arr2))\n",
    "print(\"Index of minimum value in arr3:\", np.argmin(arr3))\n",
    "\n",
    "# Linear algebra operations\n",
    "print(\"Matrix multiplication of arr4 and arr4 transposed:\\n\", np.matmul(arr4, arr4.T))\n",
    "\n",
    "# Determinant\n",
    "arr5 = np.array([[1, 2], [3, 4]])\n",
    "print(\"Determinant of arr5:\", np.linalg.det(arr5))\n",
    "\n",
    "print(\"Stacking arr1 and arr2 horizontally:\", np.hstack((arr1, arr2)))\n",
    "print(\"Stacking arr1 and arr2 vertically:\", np.vstack((arr1, arr2)))\n",
    "\n",
    "# Repeat\n",
    "print(\"Repeating elements of arr1 3 times:\", np.repeat(arr1, 3))\n",
    "\n",
    "# Tile\n",
    "print(\"Tiling arr1 2 times:\", np.tile(arr1, 2))\n",
    "\n",
    "# Clip\n",
    "print(\"Clipping arr2 between 6 and 7:\", np.clip(arr2, 6, 7))\n",
    "\n",
    "# Unique with counts\n",
    "arr4 = np.array([1, 2, 2, 3, 3, 3])\n",
    "print(\"Unique elements of arr4 with counts:\", np.unique(arr4, return_counts=True))\n",
    "\n",
    "# Random sampling\n",
    "print(\"Random sampling from arr1:\", np.random.choice(arr1, size=2, replace=False))\n",
    "\n",
    "# Finding non-zero elements\n",
    "arr5 = np.array([[0, 1, 0], [2, 0, 3], [0, 4, 0]])\n",
    "print(\"Indices of non-zero elements in arr5:\", np.nonzero(arr5))\n",
    "\n",
    "# Matrix inversion\n",
    "arr6 = np.array([[1, 2], [3, 4]])\n",
    "print(\"Inverse of arr6:\", np.linalg.inv(arr6))\n",
    "\n",
    "# Eigenvalues and eigenvectors\n",
    "eigvals, eigvecs = np.linalg.eig(arr6)\n",
    "print(\"Eigenvalues of arr6:\", eigvals)\n",
    "print(\"Eigenvectors of arr6:\", eigvecs)\n",
    "\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array([10, 20, 30, 40])\n",
    "x_new = np.array([1.5, 2.5, 3.5])\n",
    "y_new = np.interp(x_new, x, y)\n",
    "print(\"Interpolated values at x_new:\", y_new)\n",
    "\n",
    "# Correlation coefficient\n",
    "print(\"Correlation coefficient between arr1 and arr2:\", np.corrcoef(arr1, arr2)[0, 1])\n",
    "\n",
    "# Covariance matrix\n",
    "cov_matrix = np.cov(arr1, arr2)\n",
    "print(\"Covariance matrix between arr1 and arr2:\\n\", cov_matrix)\n",
    "\n",
    "# Linear regression\n",
    "slope, intercept = np.polyfit(arr1, arr2, 1)\n",
    "print(\"Slope of linear regression line:\", slope)\n",
    "print(\"Intercept of linear regression line:\", intercept)\n",
    "\n",
    "# Histogram\n",
    "arr4 = np.random.normal(0, 1, 1000)\n",
    "hist, bins = np.histogram(arr4, bins=10)\n",
    "print(\"Histogram values:\", hist)\n",
    "print(\"Histogram bins:\", bins)\n",
    "\n",
    "# Discrete Fourier Transform\n",
    "arr5 = np.array([1, 2, 3, 4])\n",
    "fft_result = np.fft.fft(arr5)\n",
    "print(\"Discrete Fourier Transform of arr5:\", fft_result)\n",
    "\n",
    "# Inverse Discrete Fourier Transform\n",
    "ifft_result = np.fft.ifft(fft_result)\n",
    "print(\"Inverse Discrete Fourier Transform of fft_result:\", ifft_result)\n",
    "\n",
    "print(\"Exponential of arr1:\", np.exp(arr1))\n",
    "\n",
    "# Logarithmic function\n",
    "print(\"Natural logarithm of arr2:\", np.log(arr2))\n",
    "\n",
    "# Trigonometric functions (cosine)\n",
    "print(\"Cosine of arr3:\", np.cos(arr3))\n",
    "\n",
    "# Hyperbolic functions (tanh)\n",
    "print(\"Hyperbolic tangent of arr1:\", np.tanh(arr1))\n",
    "\n",
    "# Rounding\n",
    "arr4 = np.array([1.25, 2.75, 3.5, 4.8])\n",
    "print(\"Rounded elements of arr4:\", np.round(arr4))\n",
    "\n",
    "# Ceiling\n",
    "print(\"Ceiling of arr4:\", np.ceil(arr4))\n",
    "\n",
    "# Floor\n",
    "print(\"Floor of arr4:\", np.floor(arr4))\n",
    "\n",
    "# Mean\n",
    "print(\"Mean of arr2:\", np.mean(arr2))\n",
    "\n",
    "# Median\n",
    "print(\"Median of arr3:\", np.median(arr3))\n",
    "\n",
    "# Standard deviation\n",
    "print(\"Standard deviation of arr1:\", np.std(arr1))\n",
    "\n",
    "# Variance\n",
    "print(\"Variance of arr2:\", np.var(arr2))\n",
    "\n",
    "# Set operations\n",
    "arr5 = np.array([1, 2, 3, 4, 5])\n",
    "arr6 = np.array([4, 5, 6, 7, 8])\n",
    "print(\"Intersection of arr5 and arr6:\", np.intersect1d(arr5, arr6))\n",
    "print(\"Union of arr5 and arr6:\", np.union1d(arr5, arr6))\n",
    "print(\"Set difference of arr5 and arr6:\", np.setdiff1d(arr5, arr6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c206041c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sport</th>\n",
       "      <th>Baseball</th>\n",
       "      <th>Basketball</th>\n",
       "      <th>Football</th>\n",
       "      <th>Soccer</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>34</td>\n",
       "      <td>52</td>\n",
       "      <td>20</td>\n",
       "      <td>44</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>34</td>\n",
       "      <td>40</td>\n",
       "      <td>58</td>\n",
       "      <td>18</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>68</td>\n",
       "      <td>92</td>\n",
       "      <td>78</td>\n",
       "      <td>62</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "sport   Baseball  Basketball  Football  Soccer  All\n",
       "gender                                             \n",
       "Female        34          52        20      44  150\n",
       "Male          34          40        58      18  150\n",
       "All           68          92        78      62  300"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#create pandas DataFrame with raw data\n",
    "df = pd.DataFrame({'gender': np.repeat(np.array(['Male', 'Female']), 150),\n",
    "                   'sport': np.repeat(np.array(['Baseball', 'Basketball', 'Football',\n",
    "                                                'Soccer', 'Baseball', 'Basketball',\n",
    "                                                'Football', 'Soccer']), \n",
    "                                    (34, 40, 58, 18, 34, 52, 20, 44))})\n",
    "\n",
    "#produce contingency table to summarize raw data\n",
    "survey_data = pd.crosstab(index=df['gender'], columns=df['sport'], margins=True)\n",
    "\n",
    "#view contingency table\n",
    "survey_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "99e3c5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_data.iloc[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5775f740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##calculate probability of being male, given that individual prefers baseball\n",
    "survey_data.iloc[1, 0] / survey_data.iloc[2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4e299e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3466666666666667"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate probability of preferring basketball, given that individual is female\n",
    "survey_data.iloc[0, 1] / survey_data.iloc[0, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "28eba447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      Name  Age  Exam_Score\n",
      "0    Alice   25          85\n",
      "1      Bob   30          90\n",
      "2  Charlie   28          75\n",
      "3    David   27          80\n",
      "4    Emily   29          95\n",
      "\n",
      "Average Exam Score: 85.0\n",
      "Median Exam Score: 85.0\n",
      "Maximum Exam Score: 95\n",
      "Minimum Exam Score: 75\n",
      "Standard Deviation of Exam Scores: 7.905694150420948\n",
      "Unique Exam Scores: [85 90 75 80 95]\n",
      "Highest Exam Score: 95\n",
      "Least Exam Score: 75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],\n",
    "    'Age': [25, 30, 28, 27, 29],\n",
    "    'Exam_Score': [85, 90, 75, 80, 95]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Calculate average exam score\n",
    "avg_score = df['Exam_Score'].mean()\n",
    "print(\"Average Exam Score:\", avg_score)\n",
    "\n",
    "# Calculate median exam score\n",
    "median_score = df['Exam_Score'].median()\n",
    "print(\"Median Exam Score:\", median_score)\n",
    "\n",
    "# Calculate maximum exam score\n",
    "max_score = df['Exam_Score'].max()\n",
    "print(\"Maximum Exam Score:\", max_score)\n",
    "\n",
    "# Calculate minimum exam score\n",
    "min_score = df['Exam_Score'].min()\n",
    "print(\"Minimum Exam Score:\", min_score)\n",
    "\n",
    "# Calculate standard deviation of exam scores\n",
    "std_dev_score = df['Exam_Score'].std()\n",
    "print(\"Standard Deviation of Exam Scores:\", std_dev_score)\n",
    "\n",
    "# Calculate unique exam scores\n",
    "unique_scores = df['Exam_Score'].unique()\n",
    "print(\"Unique Exam Scores:\", unique_scores)\n",
    "\n",
    "highest_score = df['Exam_Score'].max()\n",
    "print(\"Highest Exam Score:\", highest_score)\n",
    "\n",
    "# Calculate least exam score\n",
    "least_score = df['Exam_Score'].min()\n",
    "print(\"Least Exam Score:\", least_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a6aaf781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Probability of Scoring Above 80 Given Age Under 30: 0.5\n"
     ]
    }
   ],
   "source": [
    "students_under_30 = df[df['Age'] < 30]\n",
    "students_above_80_under_30 = students_under_30[students_under_30['Exam_Score'] > 80]\n",
    "conditional_probability = len(students_above_80_under_30) / len(students_under_30)\n",
    "\n",
    "print(\"Conditional Probability of Scoring Above 80 Given Age Under 30:\", conditional_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f68745cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that a defective widget came from Machine A: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "def probability_machine_a_given_defective(p_a, p_b_given_a, p_b_given_not_a):\n",
    "    p_b = p_a * p_b_given_a + (1 - p_a) * p_b_given_not_a\n",
    "    p_a_given_b = (p_b_given_a * p_a) / p_b\n",
    "    \n",
    "    return p_a_given_b\n",
    "\n",
    "p_a = 0.6  # Probability that widget comes from Machine A\n",
    "p_b_given_a = 0.05  # Probability of a defective widget given it came from Machine A\n",
    "p_b_given_not_a = 0.03  # Probability of a defective widget given it came from Machine B\n",
    "\n",
    "# Calculate the probability that a defective widget came from Machine A\n",
    "result = probability_machine_a_given_defective(p_a, p_b_given_a, p_b_given_not_a)\n",
    "print(\"Probability that a defective widget came from Machine A:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "28936e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Policy Parameters:\n",
      " [[ 11.97522649   5.71790464  -1.91064236 -13.99393543]\n",
      " [ -5.25239895   9.50626504   0.83077566  -2.68607266]\n",
      " [  0.34243153  -1.5566236   13.49832362 -11.49528484]\n",
      " [ -1.77547231   0.88453984  -4.03936885   7.62825852]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class TrafficSimulation:\n",
    "    def __init__(self):\n",
    "        self.state = [random.randint(0, 10) for _ in range(4)]  # Random cars at each direction\n",
    "        self.time = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # Simulate traffic flow based on action\n",
    "        # Action is which direction gets green light (0, 1, 2, or 3)\n",
    "        delay = sum(self.state) - self.state[action]  # Total delay is reduced by cars moving in the green direction\n",
    "        reward = -delay  # Negative because we want to minimize delay\n",
    "        self.state = [max(0, s - (10 if i == action else random.randint(0, 2))) for i, s in enumerate(self.state)]\n",
    "        self.time += 1\n",
    "        done = self.time > 100  # Episode ends after 100 time steps\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = [random.randint(0, 10) for _ in range(4)]\n",
    "        self.time = 0\n",
    "        return self.state\n",
    "\n",
    "def policy(state, theta):\n",
    "    logits = np.dot(state, theta)\n",
    "    return np.exp(logits) / sum(np.exp(logits))\n",
    "\n",
    "def reinforce(env, episodes=1000, learning_rate=0.01):\n",
    "    theta = np.random.rand(4, 4)  # Randomly initialized policy parameters\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        while True:\n",
    "            probs = policy(state, theta)\n",
    "            action = np.random.choice(4, p=probs)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        # Update policy after episode\n",
    "        for t in range(len(states)):\n",
    "            G_t = sum(rewards[t:])  # Total return from time t\n",
    "            grad_log_prob = -np.outer(states[t], probs)\n",
    "            grad_log_prob[:, actions[t]] += states[t]\n",
    "            theta += learning_rate * grad_log_prob * G_t\n",
    "    return theta\n",
    "\n",
    "# Initialize the traffic simulation\n",
    "env = TrafficSimulation()\n",
    "\n",
    "# Train the policy using REINFORCE\n",
    "trained_theta = reinforce(env)\n",
    "print(\"Trained Policy Parameters:\\n\", trained_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b1bc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -7\n",
      "Episode 2: Total Reward = -5\n",
      "Episode 3: Total Reward = -5\n",
      "Episode 4: Total Reward = -5\n",
      "Episode 5: Total Reward = -5\n",
      "Episode 6: Total Reward = -5\n",
      "Episode 7: Total Reward = -5\n",
      "Episode 8: Total Reward = -5\n",
      "Episode 9: Total Reward = -5\n",
      "Episode 10: Total Reward = -5\n",
      "Episode 11: Total Reward = -5\n",
      "Episode 12: Total Reward = -5\n",
      "Episode 13: Total Reward = -5\n",
      "Episode 14: Total Reward = -5\n",
      "Episode 15: Total Reward = -5\n",
      "Episode 16: Total Reward = -5\n",
      "Episode 17: Total Reward = -5\n",
      "Episode 18: Total Reward = -5\n",
      "Episode 19: Total Reward = -5\n",
      "Episode 20: Total Reward = -5\n",
      "Episode 21: Total Reward = -5\n",
      "Episode 22: Total Reward = -5\n",
      "Episode 23: Total Reward = -10\n",
      "Episode 24: Total Reward = -5\n",
      "Episode 25: Total Reward = -5\n",
      "Episode 26: Total Reward = -5\n",
      "Episode 27: Total Reward = -11\n",
      "Episode 28: Total Reward = -7\n",
      "Episode 29: Total Reward = -5\n",
      "Episode 30: Total Reward = -7\n",
      "Episode 31: Total Reward = -5\n",
      "Episode 32: Total Reward = -5\n",
      "Episode 33: Total Reward = -5\n",
      "Episode 34: Total Reward = -7\n",
      "Episode 35: Total Reward = -5\n",
      "Episode 36: Total Reward = -5\n",
      "Episode 37: Total Reward = -5\n",
      "Episode 38: Total Reward = -5\n",
      "Episode 39: Total Reward = -5\n",
      "Episode 40: Total Reward = -5\n",
      "Episode 41: Total Reward = -5\n",
      "Episode 42: Total Reward = -5\n",
      "Episode 43: Total Reward = -5\n",
      "Episode 44: Total Reward = -5\n",
      "Episode 45: Total Reward = -5\n",
      "Episode 46: Total Reward = -5\n",
      "Episode 47: Total Reward = -5\n",
      "Episode 48: Total Reward = -5\n",
      "Episode 49: Total Reward = -5\n",
      "Episode 50: Total Reward = -5\n",
      "Episode 51: Total Reward = -6\n",
      "Episode 52: Total Reward = -5\n",
      "Episode 53: Total Reward = -5\n",
      "Episode 54: Total Reward = -5\n",
      "Episode 55: Total Reward = -5\n",
      "Episode 56: Total Reward = -5\n",
      "Episode 57: Total Reward = -5\n",
      "Episode 58: Total Reward = -7\n",
      "Episode 59: Total Reward = -5\n",
      "Episode 60: Total Reward = -9\n",
      "Episode 61: Total Reward = -5\n",
      "Episode 62: Total Reward = -5\n",
      "Episode 63: Total Reward = -5\n",
      "Episode 64: Total Reward = -6\n",
      "Episode 65: Total Reward = -5\n",
      "Episode 66: Total Reward = -5\n",
      "Episode 67: Total Reward = -5\n",
      "Episode 68: Total Reward = -7\n",
      "Episode 69: Total Reward = -7\n",
      "Episode 70: Total Reward = -5\n",
      "Episode 71: Total Reward = -5\n",
      "Episode 72: Total Reward = -5\n",
      "Episode 73: Total Reward = -5\n",
      "Episode 74: Total Reward = -9\n",
      "Episode 75: Total Reward = -5\n",
      "Episode 76: Total Reward = -7\n",
      "Episode 77: Total Reward = -5\n",
      "Episode 78: Total Reward = -7\n",
      "Episode 79: Total Reward = -5\n",
      "Episode 80: Total Reward = -5\n",
      "Episode 81: Total Reward = -5\n",
      "Episode 82: Total Reward = -5\n",
      "Episode 83: Total Reward = -5\n",
      "Episode 84: Total Reward = -5\n",
      "Episode 85: Total Reward = -5\n",
      "Episode 86: Total Reward = -5\n",
      "Episode 87: Total Reward = -5\n",
      "Episode 88: Total Reward = -5\n",
      "Episode 89: Total Reward = -5\n",
      "Episode 90: Total Reward = -5\n",
      "Episode 91: Total Reward = -5\n",
      "Episode 92: Total Reward = -7\n",
      "Episode 93: Total Reward = -5\n",
      "Episode 94: Total Reward = -5\n",
      "Episode 95: Total Reward = -5\n",
      "Episode 96: Total Reward = -5\n",
      "Episode 97: Total Reward = -5\n",
      "Episode 98: Total Reward = -5\n",
      "Episode 99: Total Reward = -5\n",
      "Episode 100: Total Reward = -5\n",
      "Trained Policy Parameters:\n",
      " [[0.77854009 0.85240276 0.68759943]\n",
      " [0.32940007 0.80901575 0.74593941]\n",
      " [0.04336782 0.87055964 0.21371695]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class MazeEnvironment:\n",
    "    def __init__(self, maze, start, goal):\n",
    "        self.maze = maze\n",
    "        self.position = start\n",
    "        self.goal = goal\n",
    "        self.actions = ['forward', 'left', 'right']  # Define possible actions\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = start\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        # Simulated sensor data: random placeholder values for distances to walls\n",
    "        return np.random.randint(1, 5, size=3)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Action effect simulation (highly simplified)\n",
    "        if action == 'forward':\n",
    "            self.position = (self.position[0] + 1, self.position[1])  # Move forward\n",
    "        elif action == 'left':\n",
    "            self.position = (self.position[0], self.position[1] - 1)  # Move left\n",
    "        elif action == 'right':\n",
    "            self.position = (self.position[0], self.position[1] + 1)  # Move right\n",
    "\n",
    "        # Check boundaries and goal\n",
    "        if self.position == self.goal:\n",
    "            reward = 100  # Goal reached\n",
    "            done = True\n",
    "        elif (0 <= self.position[0] < self.maze.shape[0]) and (0 <= self.position[1] < self.maze.shape[1]):\n",
    "            reward = -1  # Penalize time steps\n",
    "            done = False\n",
    "        else:\n",
    "            reward = -5  # Hit a wall (out of bounds)\n",
    "            done = True\n",
    "            self.position = start  # Reset position\n",
    "\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "def policy(state, theta):\n",
    "    logits = np.dot(state, theta)\n",
    "    probabilities = np.exp(logits) / np.sum(np.exp(logits))\n",
    "    return probabilities\n",
    "\n",
    "def reinforce(env, episodes=100, learning_rate=0.01):\n",
    "    theta = np.random.rand(3, 3)  # Randomly initialized policy parameters\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            probs = policy(state, theta)\n",
    "            action_idx = np.random.choice(len(env.actions), p=probs)\n",
    "            action = env.actions[action_idx]\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "                break\n",
    "            state = next_state\n",
    "            # Policy update logic here (simplified for demonstration)\n",
    "    return theta\n",
    "\n",
    "# Define the maze environment\n",
    "maze_size = (10, 10)\n",
    "maze = np.zeros(maze_size)\n",
    "start = (0, 0)\n",
    "goal = (9, 9)\n",
    "env = MazeEnvironment(maze, start, goal)\n",
    "\n",
    "# Train the robot using REINFORCE\n",
    "trained_theta = reinforce(env)\n",
    "print(\"Trained Policy Parameters:\\n\", trained_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Preprocessing the image from the game\n",
    "def preprocess_frame(frame):\n",
    "    # Convert to grayscale, resize, and normalize the image\n",
    "    frame = tf.image.rgb_to_grayscale(frame)\n",
    "    frame = tf.image.resize(frame, [84, 84])\n",
    "    frame = tf.cast(frame, tf.float32) / 255.0\n",
    "    return frame\n",
    "\n",
    "# Build the model\n",
    "def build_model(input_shape, action_space):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=input_shape),\n",
    "        layers.Conv2D(64, (4, 4), strides=(2, 2), activation='relu'),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(action_space, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# REINFORCE training function\n",
    "def reinforce_train(env, model, episodes, gamma=0.99):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess_frame(state)\n",
    "        states, actions, rewards = [], [], []\n",
    "        done = False\n",
    "        while not done:\n",
    "            state = np.expand_dims(state, axis=0)  # Batch dimension\n",
    "            action_probs = model(state, training=False)\n",
    "            action = np.random.choice(env.action_space.n, p=np.squeeze(action_probs))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = preprocess_frame(next_state)\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_rewards = []\n",
    "        cumulative_reward = 0\n",
    "        for reward in reversed(rewards):\n",
    "            cumulative_reward = reward + gamma * cumulative_reward\n",
    "            discounted_rewards.insert(0, cumulative_reward)\n",
    "\n",
    "        # Update policy\n",
    "        with tf.GradientTape() as tape:\n",
    "            for i, (state, action, discounted_reward) in enumerate(zip(states, actions, discounted_rewards)):\n",
    "                state = np.squeeze(state)  # Remove batch dimension\n",
    "                action_probs = model(np.expand_dims(state, axis=0), training=True)\n",
    "                loss = -tf.math.log(action_probs[0, action]) * discounted_reward\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {sum(rewards)}\")\n",
    "\n",
    "# Main function to setup environment and train\n",
    "def main():\n",
    "    env = gym.make('Breakout-v1')\n",
    "    input_shape = (84, 84, 1)  # Processed frame shape\n",
    "    action_space = env.action_space.n\n",
    "    model = build_model(input_shape, action_space)\n",
    "    reinforce_train(env, model, episodes=500)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24bb016f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mujta\\AppData\\Local\\Temp\\ipykernel_2012\\2038495879.py:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  state = torch.FloatTensor(state).unsqueeze(0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 1 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m     reinforce(env, policy_network, episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 60\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[5], line 57\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPendulum-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     56\u001b[0m policy_network \u001b[38;5;241m=\u001b[39m PolicyNetwork()\n\u001b[1;32m---> 57\u001b[0m reinforce(env, policy_network, episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m, in \u001b[0;36mreinforce\u001b[1;34m(env, policy_network, episodes, gamma)\u001b[0m\n\u001b[0;32m     24\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 26\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     27\u001b[0m     action_pred \u001b[38;5;241m=\u001b[39m policy_network(state)\n\u001b[0;32m     28\u001b[0m     action \u001b[38;5;241m=\u001b[39m action_pred \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(action_pred) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# Add noise for exploration\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 0)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 128)  # Input size for Pendulum is 3\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output size is 1 because Pendulum has 1 continuous action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))  # Use tanh to keep the action within a reasonable range\n",
    "        return x\n",
    "\n",
    "def reinforce(env, policy_network, episodes, gamma=0.99):\n",
    "    optimizer = optim.Adam(policy_network.parameters(), lr=0.01)\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_pred = policy_network(state)\n",
    "            action = action_pred + torch.randn_like(action_pred) * 0.1  # Add noise for exploration\n",
    "\n",
    "            next_state, reward, done, _ = env.step([action.item()])  # Environment expects a list\n",
    "            log_prob = -((action_pred - action) ** 2) / (2 * 0.1**2)  # Assuming a Gaussian distribution\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_rewards = []\n",
    "        cumulative_rewards = 0\n",
    "        for reward in reversed(rewards):\n",
    "            cumulative_rewards = reward + gamma * cumulative_rewards\n",
    "            discounted_rewards.insert(0, cumulative_rewards)\n",
    "\n",
    "        # Perform policy gradient update\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss = []\n",
    "        for log_prob, discounted_reward in zip(log_probs, discounted_rewards):\n",
    "            policy_loss.append(-log_prob * discounted_reward)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {sum(rewards)}\")\n",
    "\n",
    "def main():\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    policy_network = PolicyNetwork()\n",
    "    reinforce(env, policy_network, episodes=200)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ff432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def discretize_state(state, position_bins, velocity_bins):\n",
    "    # Discretize each component of the state into its corresponding bins\n",
    "    position, velocity = state\n",
    "    discretized_position = np.digitize([position], position_bins)[0] - 1  # Convert to 0-based index\n",
    "    discretized_velocity = np.digitize([velocity], velocity_bins)[0] - 1  # Convert to 0-based index\n",
    "    return (discretized_position, discretized_velocity)\n",
    "\n",
    "def main():\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    episodes = 5000\n",
    "    learning_rate = 0.1\n",
    "    discount_factor = 0.99\n",
    "    epsilon = 0.1\n",
    "    n_bins = 20\n",
    "\n",
    "    # Create bins for discretization\n",
    "    position_bins = np.linspace(-1.2, 0.6, n_bins - 1)\n",
    "    velocity_bins = np.linspace(-0.07, 0.07, n_bins - 1)\n",
    "\n",
    "    # Initialize Q-table with small random values\n",
    "    q_table = np.random.uniform(low=-1, high=1, size=(n_bins, n_bins, env.action_space.n))\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        state = discretize_state(state, position_bins, velocity_bins)\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()  # Explore action space randomly\n",
    "            else:\n",
    "                action = np.argmax(q_table[state])  # Exploit learned values\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = discretize_state(next_state, position_bins, velocity_bins)\n",
    "\n",
    "            # Update Q-value using the Q-learning equation\n",
    "            old_value = q_table[state + (action,)]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "            new_value = (1 - learning_rate) * old_value + learning_rate * (reward + discount_factor * next_max)\n",
    "            q_table[state + (action,)] = new_value\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}: Total reward = {total_reward}, Epsilon = {epsilon}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
